# ExamPilot V2 â€” Feature Coverage Audit

> **What this is:** A verification checklist to run against your BUILT application. For each of the 18 features, it lists every algorithm, API endpoint, database table, UI component, and edge case that must exist in working code. Paste the relevant section into Claude along with your actual code files to verify coverage.
>
> **How to use:** After building each feature, paste the audit section + your code into Claude and ask: "Does this code satisfy every check? List what's missing."

---

## AUDIT METHOD

For each feature, verification happens at 4 layers:

| Layer | What to check | How to check |
|-------|--------------|-------------|
| **DB** | Tables exist, columns correct, constraints present | Run `\dt` and `\d table_name` in Supabase SQL editor |
| **API** | Endpoints exist, return correct shape, handle edge cases | curl/Postman each endpoint, verify response JSON |
| **Algorithm** | Math is correct, thresholds match spec | Unit tests with known inputs â†’ expected outputs |
| **UI** | Components render, interactions work, states display | Manual walkthrough on device/simulator |

---

## F1: ONBOARDING & STRATEGY MODE

### DB Checks
```
â–¡ user_profiles table exists with ALL columns:
  â–¡ strategy_mode (enum: balanced/aggressive/conservative/working_professional)
  â–¡ strategy_params (JSONB with 12+ keys)
  â–¡ current_mode (enum: mains/prelims/post_prelims)
  â–¡ buffer_balance (FLOAT)
  â–¡ buffer_initial (FLOAT)
  â–¡ recovery_mode_active (BOOLEAN)
  â–¡ onboarding_completed (BOOLEAN)
  â–¡ exam_date, prelims_date, daily_hours
â–¡ strategy_mode_defaults table seeded with all 4 modes Ã— all params
â–¡ user_targets table exists (daily_hours, daily_new_topics, weekly_revisions, etc.)
â–¡ persona_snapshots table exists with valid_from/valid_to (SCD Type 2)
```

### API Checks
```
â–¡ POST /api/onboarding â€” accepts questionnaire answers, returns classified mode + params
â–¡ GET /api/strategy â€” returns current mode + all active params
â–¡ POST /api/strategy/switch â€” changes mode, repopulates params, triggers recalculation
â–¡ POST /api/strategy/customize â€” overrides individual params
â–¡ POST /api/strategy/switch triggers:
  â–¡ Velocity recalculation
  â–¡ Buffer reinitialization
  â–¡ Daily plan regeneration
```

### Algorithm Checks
```
â–¡ Classification logic:
  â–¡ Working professional input â†’ WORKING_PROFESSIONAL mode
  â–¡ 7+ hours + re-attempter â†’ AGGRESSIVE
  â–¡ "Cover everything" OR first attempt â†’ CONSERVATIVE
  â–¡ Otherwise â†’ BALANCED
â–¡ Each mode populates ALL these params (no nulls):
  â–¡ buffer_deposit_rate, buffer_withdrawal_rate
  â–¡ revision_ratio_in_plan
  â–¡ fatigue_sensitivity (or fatigue_threshold)
  â–¡ burnout_threshold (or burnout_bri_threshold)
  â–¡ scope_reduction_threshold
  â–¡ velocity_target_multiplier
  â–¡ recalibration_order (array of strategy names)
  â–¡ pyq_weight_minimum (working_professional only)
  â–¡ weekend_boost (working_professional only)
```

### UI Checks
```
â–¡ Onboarding flow: 5+ screens, one question each, swipeable
â–¡ Mode recommendation card shown after classification
â–¡ "Choose different" option shows all 4 modes
â–¡ Settings screen shows current mode with "Change Mode" button
â–¡ Mode switch shows confirmation dialog before applying
```

### Edge Cases
```
â–¡ Skipping onboarding defaults to BALANCED
â–¡ Re-doing onboarding preserves existing progress data
â–¡ Custom param overrides survive mode switches (or are explicitly reset)
```

---

## F2: PYQ INTELLIGENCE LAYER

### DB Checks
```
â–¡ topics table has: pyq_weight (FLOAT), pyq_frequency (INT), pyq_trend (TEXT), pyq_years (INT[]), last_pyq_year (INT)
â–¡ pyq_data table exists: topic_id, year, paper, question_count, question_type
  â–¡ Has data for all 466 topics (at minimum importance-based seed)
  â–¡ Years span 2015-2025
â–¡ pyq_subject_stats table exists: subject_id, avg_questions_per_year, total_questions_10yr, trend
  â–¡ Has rows for all 16 subjects
```

### Algorithm Checks
```
â–¡ Recency weighting applied:
  â–¡ 2024-2025 questions weighted 1.5x
  â–¡ 2022-2023 weighted 1.2x
  â–¡ 2020-2021 weighted 1.0x
  â–¡ 2018-2019 weighted 0.8x
  â–¡ 2015-2017 weighted 0.6x
â–¡ pyq_weight normalized to 1-5 scale via percentile buckets:
  â–¡ Top 10% = 5, 70-90th = 4, 40-70th = 3, 10-40th = 2, bottom 10% = 1
â–¡ Trend calculation:
  â–¡ recent_avg (2022-25) > older_avg (2015-21) * 1.3 â†’ "rising"
  â–¡ recent_avg < older_avg * 0.7 â†’ "declining"
  â–¡ Otherwise â†’ "stable"
â–¡ total_gravity = SUM(pyq_weight) for all 466 topics (should be â‰ˆ 1420, NOT 15000+)
```

### API Checks
```
â–¡ GET /api/pyq-stats returns:
  â–¡ total_gravity, completed_gravity, remaining_gravity
  â–¡ weighted_completion_pct AND unweighted_completion_pct (both present, different values)
  â–¡ high_gravity_untouched[] (top 10 untouched high-weight topics)
  â–¡ subject_gravity[] (per-subject breakdown)
  â–¡ trending_up[] and trending_down[] (topics with rising/declining trend)
â–¡ GET /api/pyq/:topicId returns:
  â–¡ Year-by-year breakdown with papers
  â–¡ Question type distribution
```

### UI Checks
```
â–¡ PYQ flame badges visible on topic rows in syllabus map (1-5 intensity)
â–¡ Tapping flame shows "Asked X times" detail
â–¡ Dashboard toggle: topic count â†” exam-weighted progress (weighted is default)
â–¡ Trending topics card on dashboard shows 3-5 "rising" topics
â–¡ Syllabus map has a view mode to color by PYQ weight
```

---

## F3: LIVING SYLLABUS MAP

### DB Checks
```
â–¡ subjects (16 rows), chapters (93 rows), topics (466 rows) â€” all seeded
â–¡ FK chain intact: topics â†’ chapters â†’ subjects
â–¡ user_progress table: status, confidence_score, confidence_status, health_score, mock_accuracy, actual_hours, revision_count, notes
â–¡ Unique constraint on (user_id, topic_id) in user_progress
```

### UI Checks
```
â–¡ Top summary bar always visible: "187/466 topics (40%) Â· Weighted: 680/1420 (48%)"
â–¡ 3 view modes: Progress / PYQ Weight / Health â€” toggle works
â–¡ 16 subject cards render with:
  â–¡ Name, paper badges, progress ring
  â–¡ Weighted gravity progress (separate from topic count)
  â–¡ Status distribution bar (gray/blue/yellow/orange/green segments)
  â–¡ Confidence indicator (colored dot from decay engine)
â–¡ Tapping subject expands chapters (accordion)
â–¡ Tapping chapter shows topic list
â–¡ Each topic row shows:
  â–¡ Status pill (tappable to cycle)
  â–¡ PYQ flame badge
  â–¡ Confidence meter (colored bar + number)
  â–¡ Last touched ("3d ago" / "âš ï¸ 45d ago" in amber if >30 days)
â–¡ Long-press topic â†’ bottom sheet:
  â–¡ Status selector (5 pills)
  â–¡ Hours input (+0.5 increment)
  â–¡ Self-confidence (1-5 stars or similar)
  â–¡ Notes text input
  â–¡ Save â†’ optimistic UI update
â–¡ Search bar: filter by name
â–¡ Filter: by status, subject, PYQ weight range, health zone, confidence status
â–¡ Sort: by default order, PYQ weight, health score, last touched
```

### Edge Cases
```
â–¡ New user: all topics "untouched", only Blind Spots visible in radar
â–¡ Completing a topic cascades: progress ring updates â†’ chapter â†’ subject â†’ overall
â–¡ "deferred_scope" topics render grayed out (not hidden)
```

---

## F4: WEIGHTED VELOCITY ENGINE + DYNAMIC BUFFER BANK

### Algorithm Checks â€” Velocity
```
â–¡ gravity(topic) = pyq_weight (NOT pyq_weight Ã— difficulty Ã— estimated_hours)
â–¡ total_gravity â‰ˆ 1420 (verify with: SELECT SUM(pyq_weight) FROM topics)
â–¡ required_velocity = remaining_gravity / effective_study_days
â–¡ effective_study_days = days_remaining Ã— (1 - buffer_pct - revision_pct)
  â–¡ buffer_pct and revision_pct read from strategy_params (NOT hardcoded)
â–¡ actual_velocity = 0.6 Ã— velocity_7d + 0.4 Ã— velocity_14d
â–¡ velocity_ratio = actual / required
â–¡ Status thresholds: >=1.0 green, >=0.8 yellow, >=0.6 orange, <0.6 red
  (OR >=1.1 ahead, >=0.9 on_track, >=0.7 behind, else at_risk â€” either is fine, just be consistent)
â–¡ Projected completion date calculated and returned
â–¡ Both weighted_pct AND unweighted_pct in response
â–¡ Trend: 7d > 14d*1.1 â†’ improving, < 14d*0.9 â†’ declining, else stable
```

### Algorithm Checks â€” Buffer Bank
```
â–¡ Deposit rate reads from strategy_params.buffer_deposit_rate (NOT hardcoded 0.8)
  â–¡ Typical values: 0.25-0.35 depending on mode
â–¡ Withdrawal rate reads from strategy_params.buffer_withdrawal_rate
  â–¡ Typical values: 0.4-0.5
â–¡ CRITICAL: withdrawal_rate > deposit_rate (asymmetric â€” losing is harsher)
â–¡ Zero day penalty = exactly -1.0
â–¡ Exact target reward = +0.1 daily (not streak-based)
â–¡ Cap: 20% of remaining days
â–¡ Floor: -5 (NOT 0 â€” debt mode must exist)
â–¡ Buffer debt (balance < 0):
  â–¡ Status = "debt"
  â–¡ Stress signal_buffer = 0
  â–¡ Auto-triggers recalibration
  â–¡ UI shows red card with negative number
â–¡ Buffer status thresholds use initial balance as reference:
  â–¡ > initial*0.8 â†’ healthy, > initial*0.4 â†’ moderate, > 0 â†’ low, â‰¤ 0 â†’ debt
```

### DB Checks
```
â–¡ velocity_snapshots: includes weighted_completion_pct, unweighted_completion_pct, trend, projected_completion_date
â–¡ buffer_transactions: includes delta_gravity, notes columns
â–¡ user_profiles: has BOTH buffer_balance AND buffer_initial
â–¡ daily_logs: includes gravity_completed (not just topics_completed)
â–¡ streaks: study, revision, plan_completion types all tracked
```

### API Checks
```
â–¡ GET /api/velocity returns ALL: ratio, status, required, actual, weighted_pct, unweighted_pct, projected date, trend, streak
â–¡ GET /api/velocity/history?days=30 returns daily snapshots (plottable array)
â–¡ POST /api/velocity/recalculate works after topic status change
â–¡ GET /api/buffer returns: balance, initial, status, trend, last 7 transactions, totals
```

### Unit Test Cases
```
â–¡ Day 1 (no history): returns required velocity only, ratio = null or 0, streak = 0
â–¡ 0 topics ever: projected_completion = null, not division-by-zero
â–¡ Exam < 30 days: buffer_pct and revision_pct reduce automatically
â–¡ Buffer at -5: further underperformance doesn't go below -5
â–¡ Buffer at max (20% of remaining): further deposits are capped
```

---

## F5: CONFIDENCE DECAY ENGINE

### DB Checks
```
â–¡ fsrs_cards table exists with: stability, difficulty, due, state, reps, lapses, etc.
â–¡ status_changes table exists: user_id, topic_id, old_status, new_status, reason, changed_at
â–¡ confidence_snapshots table exists: user_id, topic_id, snapshot_date, confidence_score, fsrs_retrievability, mock_accuracy_factor
â–¡ subject_confidence_cache (or equivalent): user_id, subject_id, pyq_weighted_confidence
```

### Algorithm Checks
```
â–¡ Confidence uses FSRS retrievability: R = (1 + elapsed_days / (9 Ã— stability))^(-1)
â–¡ Mock accuracy adjusts confidence:
  â–¡ adjusted = fsrs_confidence Ã— (0.7 + 0.3 Ã— mock_accuracy)
  â–¡ This means: topic with 80% FSRS + 30% mock accuracy â†’ ~63 (FADING, not FRESH)
â–¡ Thresholds: >=70 fresh, >=45 fading (or 50-69), >=20 stale (or 30-49), <20 decayed (or <30)
  â–¡ Verify: which thresholds does YOUR build use? Must be consistent everywhere.
â–¡ Auto-downgrade: confidence < threshold â†’ status reverts to "first_pass"
  â–¡ This ACTUALLY CHANGES the status in user_progress
  â–¡ The old status is logged in status_changes
â–¡ Subject-level confidence = PYQ-weighted average:
  â–¡ SUM(confidence Ã— pyq_weight) / SUM(pyq_weight) for all non-untouched topics in subject
```

### Behavioral Trigger Checks
```
â–¡ Daily cron job runs and recalculates ALL confidence scores
â–¡ When topic crosses FRESH â†’ FADING:
  â–¡ A decay_revision is auto-scheduled in the daily plan
â–¡ When topic drops to STALE:
  â–¡ Priority boost of +4 in daily planner
â–¡ When topic drops to DECAYED:
  â–¡ Status auto-downgrades to first_pass
  â–¡ status_changes row is inserted
  â–¡ Notification/alert is generated
â–¡ When subject confidence < 40:
  â–¡ Alert appears in weekly review
```

### API Checks
```
â–¡ POST /api/decay/recalculate (or /api/fsrs/recalculate) â€” triggers full recalc, returns summary
â–¡ GET /api/confidence/overview â€” returns overall confidence + per-subject + distribution (fresh/fading/stale/decayed counts)
â–¡ GET /api/confidence/topic/:id/curve â€” returns projected forgetting curve points + optimal revision date
```

### UI Checks
```
â–¡ Confidence meter visible on topic rows (colored bar + number)
â–¡ Subject confidence cards on dashboard (16 cards, sortable by worst-first)
â–¡ Forgetting curve chart on topic detail (line chart with projected decline)
â–¡ Decay alert banner on daily plan: "âš ï¸ 3 topics decayed overnight"
```

---

## F6: SPACED REPETITION SCHEDULER

### Algorithm Checks
```
â–¡ Uses FSRS (ts-fsrs) for scheduling â€” NOT hand-rolled +3/+10/+30
â–¡ After first_pass: FSRS card created, first review scheduled
â–¡ Rating system: 1=Again, 2=Hard, 3=Good, 4=Easy
â–¡ FSRS calculates next due date based on rating + stability
â–¡ Decay-aware: when confidence crosses FADING, extra decay_revision is auto-inserted
  â–¡ This is SEPARATE from the FSRS-scheduled revision
â–¡ Auto-upgrade: revision_count >= 3 AND confidence >= 70 â†’ "exam_ready"
â–¡ Auto-downgrade: confidence < decay_threshold â†’ "first_pass" (from F5)
â–¡ Fast-track: mock_accuracy >= 0.8 AND revision_count >= 2 â†’ "exam_ready"
```

### DB Checks
```
â–¡ fsrs_cards: all FSRS fields present (stability, difficulty, due, state, reps, lapses, etc.)
â–¡ daily_plans has item_type values: 'revision' (FSRS-scheduled) AND 'decay_revision' (decay-triggered)
```

### API Checks
```
â–¡ POST /api/fsrs/review/:topicId â€” accepts rating (1-4), returns updated card + next due
â–¡ GET /api/revisions â€” returns: due today, overdue, upcoming 3 days, decay-triggered
  â–¡ Overdue items flagged with overdue_by_days
â–¡ GET /api/revisions/calendar?month=YYYY-MM â€” calendar view data
```

### UI Checks
```
â–¡ Revision widget in daily plan: collapsible section "ðŸ“– Revisions Due (N)"
â–¡ Each revision card: topic name, subject badge, "Rev N Â· ~15 min", quick-complete button
â–¡ Rating modal after completing revision: Again/Hard/Good/Easy buttons
â–¡ Overdue items: amber border + "X days overdue" badge
â–¡ Decay-triggered items: red indicator + "Confidence dropping" label
```

---

## F7: STRESS THERMOMETER

### Algorithm Checks
```
â–¡ 4 signals (NOT 3):
  â–¡ signal_velocity (weight 0.35): from velocity_ratio
  â–¡ signal_buffer (weight 0.25): from dynamic buffer_balance / buffer_initial
  â–¡ signal_time (weight 0.20): from weighted completion gap
  â–¡ signal_confidence (weight 0.20): from overall confidence score
â–¡ Interpolation uses piecewise-linear with multiple anchor points (NOT simple 2-point lerp):
  â–¡ Velocity: ratio 1.2â†’100, 1.0â†’80, 0.8â†’55, 0.6â†’30, 0.4â†’10, â‰¤0.2â†’0
  â–¡ OR: the LLD's simpler lerp is acceptable IF the thresholds produce similar results
â–¡ Composite: stress_score = SUM(signal Ã— weight)
â–¡ Status: >=70 green, >=45 yellow, >=25 orange, <25 red
â–¡ Buffer signal uses DYNAMIC buffer (not static percentage)
â–¡ Confidence signal pulls from decay engine's overall_confidence
```

### API Checks
```
â–¡ GET /api/stress returns: score, status, label, 4 signal breakdowns, recommendation (if not green), 7-day history
â–¡ Recommendation text is context-aware: mentions the WEAKEST signal specifically
  â–¡ Not generic "you're behind" but "Buffer consumed 60% + 8 topics decayed"
```

### UI Checks
```
â–¡ Semi-circular or vertical gauge (0-100) with color gradient
â–¡ Score number + status label ("On Track" / "At Risk")
â–¡ 4 mini signal bars (velocity, buffer, time, confidence)
â–¡ 7-day sparkline
â–¡ Recommendation card (yellow/orange/red only) with CTA to recalibration
â–¡ This is the FIRST thing on the dashboard
```

---

## F8: SMART DAILY PLANNER

### Algorithm Checks â€” Priority Scoring
```
â–¡ pyq_weight Ã— 4 (PYQ is dominant factor â€” verify it has highest coefficient)
â–¡ importance Ã— 2
â–¡ urgency Ã— 2 (subject-level, based on completion gap)
â–¡ weakness_boost: +5 for false_security, +3 for blind_spots, -3 for over_revised
  â–¡ Verify: planner actually queries weakness radar results
â–¡ decay_boost: +4 for stale, +6 for decayed confidence_status
â–¡ freshness: +3 if >7 days, +1 if 3-7 days, -2 if <3 days
â–¡ variety_bonus: +2 if different subject from previous item in today's plan
â–¡ mock_boost (if implemented): +3 for accuracy < 0.3, +2 for < 0.5
â–¡ prelims_boost (if prelims mode): +3 for prelims-relevant subjects
```

### Algorithm Checks â€” Fatigue Constraints
```
â–¡ Fatigue score calculated:
  (consecutive_study_days Ã— 10) + (avg_difficulty_3d Ã— 8) + (hours_3d / target Ã— 20) - (rest_days_7d Ã— 15)
â–¡ Fatigue > 85: FORCES light day (only revision + 1 easy topic)
â–¡ Fatigue > 70: reduces topic count by 1
â–¡ After 2 consecutive heavy days (avg difficulty >= 4): Day 3 must be <= 3 avg difficulty
â–¡ Every 6th consecutive study day: auto light day
â–¡ Max 60% of daily topics from same subject
â–¡ Subject in 3+ of last 4 plans: priority reduced by 50%
```

### Algorithm Checks â€” Plan Construction
```
â–¡ Greedy fill: sort by priority DESC, pick until hours exhausted
â–¡ Minimum 2 different subjects per plan
â–¡ Revision ratio follows strategy_params.revision_ratio_in_plan
  â–¡ e.g., BALANCED = 30% revision, PRELIMS = 70% revision
â–¡ Working Professional: pyq_weight_minimum filter applied (only weight >= 3)
â–¡ Working Professional: weekend_boost doubles capacity on Sat/Sun
â–¡ Deferred items from yesterday appear with +1 priority boost
â–¡ Decay revisions from F5 included with type = "decay_revision"
â–¡ FSRS revisions from F6 included with type = "revision"
```

### API Checks
```
â–¡ GET /api/daily-plan?date=YYYY-MM-DD â€” returns plan (auto-generates if not exists)
  â–¡ Response includes: items[], fatigue_score, fatigue_status, is_light_day, energy_level
â–¡ PATCH /api/daily-plan/items/:id â€” updates status
  â–¡ On "completed": updates user_progress + triggers velocity recalc + checks badges
  â–¡ On "deferred": adds to tomorrow with +1 priority
â–¡ POST /api/daily-plan/regenerate â€” manual regen
â–¡ Plan is deterministic: opening app multiple times on same day returns same plan
```

### UI Checks
```
â–¡ Header: "Today's Mission" + date + available hours (tappable) + energy battery + stress dot
â–¡ Topic cards: checkbox + name + chapter + subject badge + type badge (NEW/REVISION/DECAY) + PYQ flames + time
â–¡ Swipe actions: Skip / Tomorrow
â–¡ Completed items â†’ green check, move to bottom
â–¡ Light day banner: "Recovery Day ðŸŒ¿"
â–¡ Progress footer: "3/5 done" + progress bar + streak
â–¡ Stretch goals section (collapsed by default)
â–¡ All-done celebration (haptic + subtle animation)
```

---

## F9: WEAKNESS RADAR

### Algorithm Checks â€” Health Score
```
â–¡ 4 components with correct weights:
  â–¡ completion_base (0.25): in_progressâ†’20, first_passâ†’40, revisedâ†’65, exam_readyâ†’85
  â–¡ revision_score (0.20): min(revision_count / expected Ã— 100, 100)
  â–¡ accuracy_score (0.30): mock_accuracy Ã— 100 (or confidence_score if no mock data)
    â–¡ CRITICAL: mock_accuracy must be a separate signal from FSRS confidence
  â–¡ recency_score (0.25): 0-7dâ†’100, 8-14dâ†’80, 15-30dâ†’60, 31-45dâ†’35, 46-60dâ†’15, 60+â†’0
â–¡ Health zones: >=80 strong, >=60 adequate, >=40 vulnerable, >=20 weak, <20 critical
```

### Algorithm Checks â€” 3 Radar Insights
```
â–¡ "False Security": status IN (first_pass, revised) AND health_score < 40
  â–¡ Sorted by (importance Ã— pyq_weight) DESC
  â–¡ This list EXISTS and is QUERYABLE
â–¡ "Blind Spots": status = untouched AND importance >= 4
  â–¡ Sorted by (importance Ã— pyq_weight) DESC
â–¡ "Over-Revised": revision_count >= 4 AND health_score >= 80 AND importance <= 3
â–¡ These 3 lists feed INTO the daily planner:
  â–¡ false_security â†’ priority += 5
  â–¡ blind_spots â†’ priority += 3
  â–¡ over_revised â†’ priority -= 3
  â–¡ Verify: planner ACTUALLY reads these lists (not just theoretically)
```

### API Checks
```
â–¡ GET /api/weakness returns:
  â–¡ overall_health (single number)
  â–¡ zone_distribution (count + pct for each of 6 zones)
  â–¡ false_security[] (top 15)
  â–¡ blind_spots[] (top 10)
  â–¡ over_revised[] (all matching)
  â–¡ subject_health[] (16 subjects with avg health)
â–¡ GET /api/weakness/topic/:id â€” health score detail for one topic
â–¡ POST /api/weakness/recalculate â€” daily recalc
```

### UI Checks
```
â–¡ Overall health ring/gauge (0-100)
â–¡ Zone distribution bar (6 colored segments, tappable)
â–¡ 3 insight tabs: False Security / Blind Spots / Over-Revised
â–¡ Each topic in insights: name, health score, reason tag, "Add to plan" CTA
â–¡ Subject heatmap: 4Ã—4 grid colored by avg health
â–¡ "Apply to Plan" button that actually boosts priorities in planner
```

---

## F10: RECALIBRATION ENGINE

### Algorithm Checks â€” 4-Strategy Cascade
```
â–¡ Strategy 1: ABSORB â€” spread backlog over 7-14 days
  â–¡ Feasible only if gap < 10 gravity units
â–¡ Strategy 2: CONSUME BUFFERS â€” convert buffer days to study days
  â–¡ Feasible only if buffer_balance > 25% of initial
  â–¡ Creates a buffer_transaction with type "recalibration_adjustment"
â–¡ Strategy 3: INCREASE VELOCITY â€” suggest adding study hours
  â–¡ Always feasible
â–¡ Strategy 4: SCOPE REDUCTION â€” deprioritize low-weight topics
  â–¡ Targets: untouched topics with importance <= 2 AND pyq_weight <= 2
  â–¡ Sets status = "deferred_scope"
  â–¡ Feasibility depends on strategy_params.scope_reduction_threshold
â–¡ Strategy ORDER is personalized per mode:
  â–¡ Conservative: consume_buffers â†’ absorb â†’ increase_hours (NEVER reduce_scope)
  â–¡ Aggressive: absorb â†’ reduce_scope â†’ increase_hours
  â–¡ Balanced: absorb â†’ consume_buffers â†’ increase_hours â†’ reduce_scope
  â–¡ Working Professional: reduce_scope â†’ consume_buffers â†’ absorb
â–¡ Each strategy shows before/after impact numbers
```

### Trigger Checks
```
â–¡ Triggers on: velocity_ratio < 0.8 for 3 consecutive days
â–¡ Triggers on: buffer_balance consumed > 50% of initial
â–¡ Triggers on: stress_score < 45
â–¡ Triggers on: buffer_balance goes negative (from F4)
â–¡ Triggers on: manual request
â–¡ Auto-recalibration (LLD addition): tunes params with Â±10% clamps, 3-day cooldown â€” SEPARATE from strategy cascade
```

### API Checks
```
â–¡ GET /api/recalibration returns: triggered_by, gap, 4 strategies with feasibility + impact + recommended flag
â–¡ POST /api/recalibration/apply â€” applies strategy, logs event, recalculates velocity/stress/buffer
â–¡ recalibration_log table: triggered_at, triggered_by, strategy_chosen, stress_before/after, buffer_before/after
```

---

## F11: FATIGUE & BURNOUT GUARDIAN

### Algorithm Checks â€” Fatigue (daily)
```
â–¡ Formula: (consecutive_days Ã— 10) + (avg_diff_3d Ã— 8) + (hours_3d / target Ã— 20) - (rest_7d Ã— 15)
â–¡ > 85: force light day
â–¡ > 70: reduce topic count
â–¡ < 30: can handle heavy day
â–¡ Light day: revisions + 1 easy topic, 60% hours, "Recovery Day ðŸŒ¿"
â–¡ Heavy day limit: max 2 consecutive days with avg difficulty >= 4
```

### Algorithm Checks â€” Burnout Risk Index (multi-day)
```
â–¡ BRI calculated from 4 signals:
  â–¡ Stress persistence (0.30)
  â–¡ Buffer hemorrhage (0.25)
  â–¡ Velocity collapse (0.25)
  â–¡ Engagement decay (0.20)
â–¡ BRI thresholds adjusted by strategy_params.burnout_threshold:
  â–¡ Conservative: triggers at 65 (earlier)
  â–¡ Aggressive: triggers at 80 (later)
â–¡ Recovery Mode triggers: BRI > threshold for 2 consecutive days
```

### Recovery Mode Checks
```
â–¡ Duration: 5-7 days
â–¡ During recovery:
  â–¡ Daily plan reduced to 50% (only revision + 1 easy)
  â–¡ Buffer consumption PAUSED (no withdrawals)
  â–¡ Velocity target FROZEN (no "falling behind")
  â–¡ Stress thermometer shows special recovery state
â–¡ Exit: 5 days + BRI below 50, OR 7 days auto, OR manual (with warning)
â–¡ Post-recovery ramp-up: Day 1 = 70%, Day 2 = 85%, Day 3 = 100%
```

### API Checks
```
â–¡ GET /api/burnout returns: bri, status, 4 signal scores, recommendation, 7-day history
â–¡ POST /api/burnout/recovery/start â€” activates recovery
â–¡ POST /api/burnout/recovery/end â€” exits with ramp-up schedule
â–¡ burnout_snapshots table records daily BRI
â–¡ recovery_log table records start/end/trigger/exit_reason
```

### UI Checks
```
â–¡ BRI indicator on dashboard (heart-rate icon, colored)
â–¡ Recovery banner: "ðŸŒ¿ Recovery Mode â€” Day 3/5" with "Exit Early" button
â–¡ Early warning modal (BRI 50-75): "Stressed? Activate light week?" (max once/day)
â–¡ Energy battery icon on daily plan header
```

---

## F12: WEEKLY REVIEW RITUAL

### Content Checks (response must include ALL of these)
```
â–¡ Performance: topics done vs target, hours vs target, days active/missed
â–¡ Gravity: gravity completed vs target (weighted metric)
â–¡ Velocity trend: this week vs last, % change, direction
â–¡ Confidence trend: start vs end, topics improved vs decayed, subjects at risk
â–¡ Weakness radar changes: false_security count change, overall_health change
â–¡ Buffer bank: start vs end balance, net deposited/withdrawn
â–¡ Burnout: avg BRI, peak BRI, recovery triggered (yes/no), light days taken
â–¡ Subject coverage: touched, untouched this week, untouched >14 days
â–¡ Stress trend: start vs end, status
â–¡ Gamification: WES or grade, streaks, badges earned, next milestone
â–¡ Next week recommendation: priority subjects, revision load, topic target, reason
â–¡ Wins (always listed FIRST, never start with negatives)
â–¡ Areas to improve (gentle language, with actionable CTAs)
```

### API/DB Checks
```
â–¡ GET /api/weekly-review?week_start=YYYY-MM-DD â€” returns complete review
â–¡ weekly_reviews table caches the review (JSONB) â€” not regenerated on repeat access
â–¡ Push notification on Sunday at configured time
â–¡ "Accept Plan" button pre-loads priorities into next week's planner
```

---

## F13: MOCK TEST INTEGRATION

### DB Checks
```
â–¡ mock_tests: user_id, test_name, test_date, total_questions, correct, incorrect, score, source
â–¡ mock_questions (optional for detailed entry): mock_test_id, topic_id, subject_id, is_correct, difficulty
â–¡ mock_topic_accuracy: user_id, topic_id, total_questions, correct, accuracy, trend
  â–¡ UNIQUE on (user_id, topic_id)
â–¡ mock_subject_accuracy: user_id, subject_id, total/correct/accuracy, tests_count, trend
â–¡ topic_keyword_mappings (for CSV fuzzy matching cache)
```

### Feedback Loop Checks (CRITICAL â€” this closes the intelligence loop)
```
â–¡ After mock import: mock_accuracy is written to user_progress for affected topics
â–¡ After mock import: confidence recalculation triggered (F5)
â–¡ After mock import: health scores recalculated (F9)
â–¡ Low subject accuracy (< 0.5): urgency boost +2 in daily planner
â–¡ Low topic accuracy (< 0.3): added to false_security list in weakness radar
â–¡ Low topic accuracy (< 0.3): immediate revision scheduled (overrides FSRS)
â–¡ Score trend calculation: linear regression across last 5+ mocks
```

### API/UI Checks
```
â–¡ POST /api/mocks â€” create mock (quick or detailed entry)
â–¡ GET /api/mocks/analytics â€” score trends, subject accuracy, weakest topics
â–¡ Mock entry: quick (just scores) + detailed (per question) + CSV import
â–¡ Score trend chart: line over time with green/yellow/red zones
â–¡ Subject accuracy grid: 16 tiles, colored by accuracy
â–¡ Weakest topics alert with "Add to plan" CTA
```

---

## F14: PRELIMS/MAINS MODE TOGGLE

### DB Checks
```
â–¡ mode_config table exists (seeded): mode, subject_id, is_active, importance_modifier, revision_ratio
  â–¡ Prelims mode: Ethics/Internal Security/World History â†’ is_active = false
  â–¡ Prelims mode: Environment/Science/Art & Culture â†’ importance_modifier = +1
  â–¡ Prelims mode: revision_ratio = 0.70
â–¡ user_profiles.current_mode: mains/prelims/post_prelims
```

### Behavior Checks
```
â–¡ Switching to Prelims:
  â–¡ Paused subjects are grayed out in syllabus map (NOT deleted)
  â–¡ Daily planner filters out paused subjects
  â–¡ Boosted subjects get importance +1
  â–¡ Revision ratio shifts to 70/30
  â–¡ Mock test slots appear (2/week)
â–¡ Switching to Post-Prelims:
  â–¡ Paused subjects reactivated
  â–¡ Answer writing focus (if tracked)
â–¡ Switching back to Mains: everything returns to normal
â–¡ Velocity engine recalculates with reduced scope in Prelims mode
â–¡ Weakness radar only shows active subjects
```

### API Checks
```
â–¡ POST /api/mode/switch â€” changes mode, logs, regenerates plan
â–¡ GET /api/mode/preview?mode=prelims â€” shows diff WITHOUT applying
```

---

## F15: "WHAT IF" SIMULATOR

### Algorithm Checks â€” ALL 5 Scenarios Must Work
```
â–¡ Scenario 1: "Take N days off" (N = 1-14)
  â–¡ Calculates: new effective days, new required velocity, new stress, buffer consumed
â–¡ Scenario 2: "Change hours by X" (X = -3 to +3)
  â–¡ Calculates: new daily capacity, new velocity ratio, new projected date
â–¡ Scenario 3: "Drop importance â‰¤ N topics" (N = 1-3)
  â–¡ Calculates: topics removed, gravity removed, new velocity, hours saved
â–¡ Scenario 4: "Focus only on subject X for N days"
  â–¡ Calculates: subject progress projection, other subjects' confidence decay
â–¡ Scenario 5: "Exam postponed by N days" (N = 7-90)
  â–¡ Calculates: new pace, new buffer, new stress

â–¡ ALL scenarios are READ-ONLY (no data written)
â–¡ Each returns: current metrics, simulated metrics, delta, verdict (green/yellow/red)
```

### API Check
```
â–¡ POST /api/simulator/run â€” body: {scenario, params} â€” returns simulation_result
```

### UI Check
```
â–¡ 5 scenario cards to choose from
â–¡ Parameter input (slider/number) for each
â–¡ Before/after comparison cards (color-coded)
â–¡ Verdict banner: "Safe to take 5 days off ðŸŸ¢"
```

---

## F16: CURRENT AFFAIRS TRACKER

### Checks
```
â–¡ ca_daily_logs: user_id, log_date, hours_spent, completed, subject_ids[]
â–¡ ca_streaks: current/best streak tracking
â–¡ Daily toggle: "Did you read today?" Yes/No â†’ log hours + subject tags
â–¡ Subject distribution: pie chart of which subjects CA covers
â–¡ Alert for undercovered subjects
â–¡ Monthly heatmap: calendar grid (green=done, gray=missed)
â–¡ Prelims mode: shows "Cover 12 months of CA. Done: X hrs of Y hrs"
```

---

## F17: GAMIFICATION LAYER

### Checks
```
â–¡ Weekly Execution Score (WES) calculated from 4 components:
  â–¡ plan_adherence (0.35), velocity_factor (0.25), revision_consistency (0.20), consistency_factor (0.20)
  â–¡ Grades: 90-100=S, 80-89=A, 70-79=B, 60-69=C, <60=D
â–¡ OR XP system from LLD â€” either approach is fine, but ONE must exist
â–¡ 3 streak types tracked: study, revision, plan_completion
â–¡ 15+ badge definitions seeded
â–¡ Badge unlock conditions actually checked on relevant events
â–¡ Badges fire exactly once (no duplicate unlocks)
â–¡ Celebrations: haptic on plan complete, slide-up card on badge, no sound/confetti
â–¡ UI is premium/dashboard aesthetic (NOT childish gamification)
â–¡ WES or XP level visible on dashboard
â–¡ Badge gallery in settings/profile (earned = color, locked = gray)
```

---

## F18: STRATEGIC BENCHMARK LAYER

### Current State Check (LLD has self-benchmark, prompts have peer benchmark)
```
For MVP (self-benchmark â€” what LLD implements):
â–¡ Composite readiness score calculated from: coverage, confidence, weakness, consistency, velocity
â–¡ Status: exam_ready / on_track / needs_work / at_risk
â–¡ History tracked over time

For Phase 2 (peer benchmark â€” from prompts):
â–¡ benchmark_cohorts table: exam_year, creation_month
â–¡ benchmark_snapshots: aggregated percentiles (p25/p50/p75/p90)
â–¡ Opt-in/opt-out mechanism
â–¡ Minimum 20 users per cohort
â–¡ "Successful aspirant" reference line (seeded even without users)
â–¡ Percentile radar chart (6 axes)
â–¡ Anti-toxic: never "bottom 10%", always paired with action
â–¡ > 5 checks/day: "Focus on your plan today"

â–¡ DECISION: Document clearly which version is built and which is deferred
```

---

## STRUCTURAL CHECKS (Cross-Cutting)

### Cron Jobs
```
â–¡ Daily 2:00 AM: Confidence recalculation (F5 decay triggers)
â–¡ Daily 2:15 AM: Health score recalculation (F9 weakness radar)
â–¡ Daily 2:30 AM: Velocity snapshot + buffer transaction (F4)
â–¡ Daily 2:45 AM: Burnout snapshot + BRI calculation (F11)
â–¡ Daily 3:00 AM: Benchmark snapshot (F18)
â–¡ Sunday 7:00 PM: Weekly review generation + push notification (F12)
â–¡ Implementation: Supabase Edge Functions with pg_cron OR external scheduler
â–¡ Each cron is idempotent (running twice doesn't double-count)
```

### Notification Events
```
â–¡ Notification infrastructure exists (table or service)
â–¡ These events generate notifications:
  â–¡ Recalibration triggered (F10)
  â–¡ Weekly review ready â€” Sunday (F12)
  â–¡ Recovery mode suggestion â€” BRI high (F11)
  â–¡ Topic decay alert â€” "3 topics decayed" (F5)
  â–¡ Streak milestone â€” 7/14/30/100 days (F17)
  â–¡ Badge unlocked (F17)
  â–¡ Mock score improvement (F13)
```

### Supabase RLS Policies
```
â–¡ Every table with user_id has RLS enabled
â–¡ Policy: users can only SELECT/INSERT/UPDATE/DELETE their own rows
â–¡ auth.uid() = user_id pattern on all policies
â–¡ Seeded tables (strategy_mode_defaults, badges, mode_config) are read-only for all
```

### Data Cascade on Topic Completion
```
When a topic is marked "completed" (first_pass or higher):
â–¡ user_progress updated
â–¡ FSRS card created (if first time)
â–¡ Velocity recalculated
â–¡ Buffer transaction processed
â–¡ Streak updated
â–¡ Daily plan item marked complete
â–¡ Gamification check (XP/badge)
â–¡ Progress aggregation cascades up (chapter â†’ subject â†’ overall)
â–¡ Health score will update on next cron
â–¡ Confidence score will update on next cron

Verify: ALL of these actually happen in your completion handler (not just some).
```

---

## RUNNING THE AUDIT

### Step 1: DB Audit
```sql
-- Run in Supabase SQL editor
-- Lists all tables and their columns
SELECT table_name, column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'public' 
ORDER BY table_name, ordinal_position;
```
Compare output against every DB Check above.

### Step 2: API Audit
```bash
# Run against your backend
# Replace BASE_URL and TOKEN
for endpoint in \
  "GET /api/velocity" \
  "GET /api/buffer" \
  "GET /api/stress" \
  "GET /api/daily-plan" \
  "GET /api/weakness" \
  "GET /api/confidence/overview" \
  "GET /api/pyq-stats" \
  "GET /api/burnout" \
  "GET /api/weekly-review" \
  "GET /api/gamification" \
  "GET /api/benchmark" \
  "GET /api/mocks/analytics" \
  "GET /api/ca/stats"; do
  echo "--- $endpoint ---"
  curl -s -H "Authorization: Bearer $TOKEN" "$BASE_URL$endpoint" | jq 'keys'
done
```
Compare response keys against every API Check above.

### Step 3: Algorithm Audit
Paste each algorithm check section + your service code into Claude:
"Here is my velocity.service.ts. Does it satisfy every algorithm check in this list? Show me what's missing."

### Step 4: UI Audit
Walk through each screen on your device/simulator with the UI Checks open. Screenshot each screen and verify every element exists.
